{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Verifying Previously Normalized Image Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://becominghuman.ai/understand-and-apply-capsnet-on-traffic-sign-classification-a592e2d4a4ea\n",
    "\n",
    "https://www.zhihu.com/question/67287444\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/29435406\n",
    "\n",
    "https://github.com/naturomics/CapsNet-Tensorflow\n",
    "\n",
    "https://github.com/XifengGuo/CapsNet-Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "AugumentedDataSetName='TrafficSignNormalized_0.5.pickle'\n",
    "with open(AugumentedDataSetName, mode='rb') as f:\n",
    "    AugumentedData = pickle.load(f)\n",
    "    \n",
    "X_train, y_train = AugumentedData['X_train'], AugumentedData['y_train']\n",
    "X_valid, y_valid = AugumentedData['X_valid'], AugumentedData['y_valid']\n",
    "X_test, y_test = AugumentedData['X_test'], AugumentedData['y_test']\n",
    "AugumentedData=None\n",
    "\n",
    "# TODO: Number of training examples\n",
    "n_train = len(X_train)\n",
    "\n",
    "# TODO: Number of validation examples\n",
    "n_validation = len(X_valid)\n",
    "\n",
    "# TODO: Number of testing examples.\n",
    "n_test = len(X_test)\n",
    "\n",
    "# TODO: What's the shape of an traffic sign image?\n",
    "image_shape = (X_train.shape)[1:4]\n",
    "image_width = image_shape[0]\n",
    "image_height = image_shape[1]\n",
    "image_channel = image_shape[2]\n",
    "\n",
    "# TODO: How many unique classes/labels there are in the dataset.\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of validation examples =\", n_validation)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "print(\"Image data shape =\", image_width,image_height,image_channel)\n",
    "print(\"Number of classes =\", n_classes)\n",
    "\n",
    "X_train+=0.5\n",
    "X_valid+=0.5\n",
    "X_test+=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "X_train, y_train = shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data exploration visualization code goes here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "# Visualizations will be shown in the notebook.\n",
    "%matplotlib inline\n",
    "\n",
    "def GetLabelImage(index): \n",
    "    path=\"./Data\"\n",
    "    image=cv2.imread(path+'/signnames_all.jpg',1)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    H, W, _ = image.shape\n",
    "    dY = H/7.\n",
    "    dX = W/7.105\n",
    "    y = index//7\n",
    "    x = index%7 \n",
    "    labelImage = image[round(y*dY):round(y*dY+dY), round(x*dX):round(x*dX+dX),:]\n",
    "    labelImage = cv2.resize(labelImage, (0,0), fx=32./dX, fy=32./dY,)\n",
    "    labelImage = labelImage.astype(float)/255\n",
    "    return labelImage\n",
    "\n",
    "def InsertSubimage(image, subImage, y, x):\n",
    "    h, w, _ = subImage.shape\n",
    "    image[y:y+h, x:x+w, :]=subImage\n",
    "    return image\n",
    "\n",
    "def CreateDataSummary(images, labels, sampleNumber, multiplier):\n",
    "    imageShape = (images.shape)[1:4]\n",
    "    all_labels=np.unique(labels).astype(int)\n",
    "    classNumber = np.amax(all_labels)+1\n",
    "    subimageWidth=imageShape[1]\n",
    "    subimageHeight=imageShape[0];\n",
    "    resultImageWidth=subimageWidth*(classNumber);\n",
    "    resultImageHeight=subimageHeight*sampleNumber;\n",
    "    resultImageChanel=imageShape[2];\n",
    "    resultImage = 1.0*np.ones(shape=(resultImageHeight,resultImageWidth,resultImageChanel),dtype=np.float32)\n",
    "    cv2.putText(resultImage, \"Training data histogram\", (int(subimageWidth*0.01), int(resultImageHeight*0.05)),cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,0),2)\n",
    "    for index in all_labels:\n",
    "        index=int(index)\n",
    "        labelImage = GetLabelImage(index)\n",
    "        InsertSubimage(resultImage, labelImage, resultImageHeight-subimageHeight*2, subimageWidth*index)\n",
    "        indexList = list(np.where(labels== index)[0])\n",
    "        count=len(indexList)\n",
    "        if(count>0):\n",
    "            meanImage = np.average(images[indexList], axis=0)\n",
    "            InsertSubimage(resultImage, meanImage, resultImageHeight-subimageHeight*3, subimageWidth*index)\n",
    "        totalDisplaySample=sampleNumber*multiplier;\n",
    "        percentage = float(count)/float(len(labels))\n",
    "        numberDisplaySample=int(totalDisplaySample*count/len(labels))\n",
    "        for i in range(numberDisplaySample):\n",
    "            sampleImage=images[np.random.choice(indexList)]\n",
    "            if(resultImageHeight-subimageHeight*(7+i)<0):\n",
    "                break\n",
    "            else:\n",
    "                InsertSubimage(resultImage, sampleImage, resultImageHeight-subimageHeight*(3+i), subimageWidth*index)\n",
    "        cv2.putText(resultImage, '{:.0%}'.format(percentage), (int(subimageWidth*index), int(resultImageHeight-subimageHeight*0.5)),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,0,0),2)\n",
    "    cv2.line(resultImage,(0,resultImageHeight-subimageHeight*3),(resultImageWidth,resultImageHeight-subimageHeight*3),(0,0,0.5),3)\n",
    "    #cv2.imwrite(path+'/train_data_summary.jpg',cv2.cvtColor(resultImage, cv2.COLOR_BGR2RGB))\n",
    "    return resultImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Origional Dataset:\", n_train,\"Entire Dataset:\",len(X_train))\n",
    "#resultImage=CreateDataSummary(X_train,y_train,sampleNumber=30, multiplier=64)\n",
    "#plt.rcParams[\"figure.figsize\"] = (20,30)\n",
    "#plt.imshow(resultImage)\n",
    "#plt.axis('off') \n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Build Capsnet Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Images\n",
    "Let's start by creating a placeholder for the input images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(shape=[None, image_width, image_height, image_channel], dtype=tf.float32, name=\"X\")\n",
    "print(\"image_width=\",image_width)\n",
    "print(\"image_height=\",image_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLu Conv1\n",
    "The first layer is a standard convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1_params = {\n",
    "    \"filters\": 256,   #Configureable\n",
    "    \"kernel_size\": 9, #Configurable\n",
    "    \"strides\": 1,     #Configurable\n",
    "    \"padding\": \"valid\",\n",
    "    \"activation\": tf.nn.relu,\n",
    "}\n",
    "conv1_raw = tf.layers.conv2d(X, name=\"conv1\", **conv1_params)\n",
    "conv1_keep_prob = tf.placeholder(tf.float32)\n",
    "conv1 = tf.nn.dropout(conv1_raw, conv1_keep_prob)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primary Capsules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capsule_layer(input_layer,kernel_size=6,strides=2,\n",
    "                  caps1_n_maps=16,caps1_n_dims=5,\n",
    "                 caps2_n_caps=43,caps2_n_dims=32):\n",
    "    \n",
    "    def squash(s, axis=-1, epsilon=1e-7, name=None):\n",
    "        with tf.name_scope(name, default_name=\"squash\"):\n",
    "            squared_norm = tf.reduce_sum(tf.square(s), axis=axis,\n",
    "                                         keep_dims=True)\n",
    "            safe_norm = tf.sqrt(squared_norm + epsilon)\n",
    "            squash_factor = squared_norm / (1. + squared_norm)\n",
    "            unit_vector = s / safe_norm\n",
    "            return squash_factor * unit_vector\n",
    "        \n",
    "    def primary_capsule(input_layer,kernel_size,strides,caps1_n_maps,caps1_n_dims):\n",
    "        conv2_params = {\n",
    "            \"filters\": caps1_n_maps * caps1_n_dims, # 256 convolutional filters\n",
    "            \"kernel_size\": kernel_size, #Configureable //need to change to 5\n",
    "            \"strides\": strides,     #Configureable\n",
    "            \"padding\": \"valid\",\n",
    "            \"activation\": tf.nn.relu\n",
    "        }\n",
    "        \n",
    "        input_width = (int)(input_layer.shape[2])\n",
    "        input_height = (int)(input_layer.shape[1])\n",
    "        caps1_n_caps = caps1_n_maps * input_width * input_height  #Primary Capsule Unit Count\n",
    "        print(caps1_n_caps)\n",
    "        conv2 = tf.layers.conv2d(input_layer, **conv2_params)\n",
    "        caps1_raw = tf.reshape(conv2, [-1, caps1_n_caps, caps1_n_dims])\n",
    "        caps1_output = squash(caps1_raw, )\n",
    "        return caps1_output\n",
    "    \n",
    "    def digit_capsule(primary_layer,caps2_n_caps=43,caps2_n_dims=32):\n",
    "        init_sigma = 0.01\n",
    "        batch_size = tf.shape(primary_layer)[0]\n",
    "        caps1_n_caps = (int)(primary_layer.shape[1])\n",
    "       \n",
    "        W_init = tf.random_normal(\n",
    "        shape=(1, caps1_n_caps, caps2_n_caps, caps2_n_dims, caps1_n_dims),\n",
    "                stddev=init_sigma, dtype=tf.float32)\n",
    "        W = tf.Variable(W_init)\n",
    "       \n",
    "        W_tiled = tf.tile(W, [batch_size, 1, 1, 1, 1])\n",
    "        \n",
    "        caps1_output_expanded = tf.expand_dims(primary_layer, -1)\n",
    "        caps1_output_tile = tf.expand_dims(caps1_output_expanded, 2)\n",
    "        caps1_output_tiled = tf.tile(caps1_output_tile, [1, 1, caps2_n_caps, 1, 1])\n",
    "        caps2_predicted = tf.matmul(W_tiled, caps1_output_tiled)\n",
    "        \n",
    "        #Routing by agreement\n",
    "        raw_weights = tf.zeros([batch_size, caps1_n_caps, caps2_n_caps, 1, 1],dtype=np.float32)\n",
    "        #Round 1\n",
    "        routing_weights = tf.nn.softmax(raw_weights, dim=2)\n",
    "        weighted_predictions = tf.multiply(routing_weights, caps2_predicted)\n",
    "        weighted_sum = tf.reduce_sum(weighted_predictions, axis=1, keep_dims=True)\n",
    "        caps2_output_round_1 = squash(weighted_sum, axis=-2)\n",
    "        #Round 2\n",
    "        caps2_output_round_1_tiled = tf.tile(caps2_output_round_1, [1, caps1_n_caps, 1, 1, 1])\n",
    "        agreement = tf.matmul(caps2_predicted, caps2_output_round_1_tiled, transpose_a=True)\n",
    "        raw_weights_round_2 = tf.add(raw_weights, agreement)\n",
    "        \n",
    "        routing_weights_round_2 = tf.nn.softmax(raw_weights_round_2,dim=2)\n",
    "        weighted_predictions_round_2 = tf.multiply(routing_weights_round_2,caps2_predicted)\n",
    "        weighted_sum_round_2 = tf.reduce_sum(weighted_predictions_round_2,axis=1, keep_dims=True)\n",
    "        caps2_output_round_2 = squash(weighted_sum_round_2,axis=-2)\n",
    "        return caps2_output_round_2\n",
    "    \n",
    "    primary_layer = primary_capsule(input_layer,kernel_size,strides,caps1_n_maps,caps1_n_dims)\n",
    "    print(primary_layer)\n",
    "    digit_layer = digit_capsule(primary_layer,caps2_n_caps,caps2_n_dims)\n",
    "    return digit_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "caps2_output = capsule_layer(conv1,kernel_size=6,strides=2,caps1_n_maps=16,caps1_n_dims=5,\n",
    "                 caps2_n_caps=43,caps2_n_dims=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimated Class Probabilities (Length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_norm(s, axis=-1, epsilon=1e-7, keep_dims=False, name=None):\n",
    "    with tf.name_scope(name, default_name=\"safe_norm\"):\n",
    "        squared_norm = tf.reduce_sum(tf.square(s), axis=axis,\n",
    "                                     keep_dims=keep_dims)\n",
    "        return tf.sqrt(squared_norm + epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = safe_norm(caps2_output, axis=-2, name=\"y_proba\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_proba_argmax = tf.argmax(y_proba, axis=2, name=\"y_proba\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba_argmax = tf.argmax(y_proba, axis=2, name=\"y_proba\")\n",
    "print(y_proba_argmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tf.squeeze(y_proba_argmax, axis=[1,2], name=\"y_pred\")\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, we are now ready to define the training operations, starting with the losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will need a placeholder for the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.placeholder(shape=[None], dtype=tf.int64, name=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Margin loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper uses a special margin loss to make it possible to detect two or more different digits in each image:\n",
    "\n",
    "$ L_k = T_k \\max(0, m^{+} - \\|\\mathbf{v}_k\\|)^2 - \\lambda (1 - T_k) \\max(0, \\|\\mathbf{v}_k\\| - m^{-})^2$\n",
    "\n",
    "* $T_k$ is equal to 1 if the digit of class $k$ is present, or 0 otherwise.\n",
    "* In the paper, $m^{+} = 0.9$, $m^{-} = 0.1$ and $\\lambda = 0.5$.\n",
    "* Note that there was an error in the video (at 15:47): the max operations are squared, not the norms. Sorry about that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_plus = 0.9\n",
    "m_minus = 0.1\n",
    "lambda_ = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `y` will contain the digit classes, from 0 to 9, to get $T_k$ for every instance and every class, we can just use the `tf.one_hot()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = tf.one_hot(y, depth=caps2_n_caps, name=\"T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caps2_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 16D output vectors are in the second to last dimension, so let's use the `safe_norm()` function with `axis=-2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caps2_output_norm = safe_norm(caps2_output, axis=-2, keep_dims=True, name=\"caps2_output_norm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute $\\max(0, m^{+} - \\|\\mathbf{v}_k\\|)^2$, and reshape the result to get a simple matrix of shape (_batch size_, 10):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "present_error_raw = tf.square(tf.maximum(0., m_plus - caps2_output_norm),\n",
    "                              name=\"present_error_raw\")\n",
    "present_error = tf.reshape(present_error_raw, shape=(-1, n_classes),\n",
    "                           name=\"present_error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's compute $\\max(0, \\|\\mathbf{v}_k\\| - m^{-})^2$ and reshape it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "absent_error_raw = tf.square(tf.maximum(0., caps2_output_norm - m_minus),\n",
    "                             name=\"absent_error_raw\")\n",
    "absent_error = tf.reshape(absent_error_raw, shape=(-1, n_classes),\n",
    "                          name=\"absent_error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to compute the loss for each instance and each digit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = tf.add(T * present_error, lambda_ * (1.0 - T) * absent_error, name=\"L\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can sum the digit losses for each instance ($L_0 + L_1 + \\cdots + L_9$), and compute the mean over all instances. This gives us the final margin loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin_loss = tf.reduce_mean(tf.reduce_sum(L, axis=1), name=\"margin_loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add a decoder network on top of the capsule network. It is a regular 3-layer fully connected neural network which will learn to reconstruct the input images based on the output of the capsule network. This will force the capsule network to preserve all the information required to reconstruct the digits, across the whole network. This constraint regularizes the model: it reduces the risk of overfitting the training set, and it helps generalize to new digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper mentions that during training, instead of sending all the outputs of the capsule network to the decoder network, we must send only the output vector of the capsule that corresponds to the target digit. All the other output vectors must be masked out. At inference time, we must mask all output vectors except for the longest one, i.e., the one that corresponds to the predicted digit. You can see this in the paper's figure 2 (at 18:15 in the video): all output vectors are masked out, except for the reconstruction target's output vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a placeholder to tell TensorFlow whether we want to mask the output vectors based on the labels (`True`) or on the predictions (`False`, the default):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_with_labels = tf.placeholder_with_default(False, shape=(), name=\"mask_with_labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use `tf.cond()` to define the reconstruction targets as the labels `y` if `mask_with_labels` is `True`, or `y_pred` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_targets = tf.cond(mask_with_labels, # condition\n",
    "                                 lambda: y,        # if True\n",
    "                                 lambda: y_pred,   # if False\n",
    "                                 name=\"reconstruction_targets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `tf.cond()` function expects the if-True and if-False tensors to be passed _via_ functions: these functions will be called just once during the graph construction phase (not during the execution phase), similar to `tf.while_loop()`. This allows TensorFlow to add the necessary operations to handle the conditional evaluation of the if-True or if-False tensors. However, in our case, the tensors `y` and `y_pred` are already created by the time we call `tf.cond()`, so unfortunately TensorFlow will consider both `y` and `y_pred` to be dependencies of the `reconstruction_targets` tensor. The `reconstruction_targets` tensor will end up with the correct value, but:\n",
    "1. whenever we evaluate a tensor that depends on `reconstruction_targets`, the `y_pred` tensor will be evaluated (even if `mask_with_layers` is `True`). This is not a big deal because computing `y_pred` adds no computing overhead during training, since we need it anyway to compute the margin loss. And during testing, if we are doing classification, we won't need reconstructions, so `reconstruction_targets` won't be evaluated at all.\n",
    "2. we will always need to feed a value for the `y` placeholder (even if `mask_with_layers` is `False`). This is a bit annoying, but we can pass an empty array, because TensorFlow won't use it anyway (it just does not know it yet when it checks for dependencies)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the reconstruction targets, let's create the reconstruction mask. It should be equal to 1.0 for the target class, and 0.0 for the other classes, for each instance. For this we can just use the `tf.one_hot()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_mask = tf.one_hot(reconstruction_targets,\n",
    "                                 depth=caps2_n_caps,\n",
    "                                 name=\"reconstruction_mask\")\n",
    "print(reconstruction_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare this to the shape of `caps2_output`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caps2_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mmh, its shape is (batch size, 1, 10, 16, 1). We want to multiply it by the reconstruction_mask, but the shape of the reconstruction_mask is (batch size, 10). We must reshape it to (batch size, 1, 10, 1, 1) to make multiplication possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_mask_reshaped = tf.reshape(reconstruction_mask, [-1, 1, caps2_n_caps, 1, 1]\n",
    "                                          ,name=\"reconstruction_mask_reshaped\")\n",
    "print(reconstruction_mask_reshaped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At last! We can apply the mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caps2_output_masked = tf.multiply(\n",
    "    caps2_output, reconstruction_mask_reshaped,\n",
    "    name=\"caps2_output_masked\")\n",
    "print(caps2_output_masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last reshape operation to flatten the decoder's inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = tf.reshape(caps2_output_masked,\n",
    "                           [-1, caps2_n_caps * caps2_n_dims],\n",
    "                           name=\"decoder_input\")\n",
    "print(decoder_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build the decoder. It's quite simple: two dense (fully connected) ReLU layers followed by a dense output sigmoid layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden1 = 512 #Configurable\n",
    "n_hidden2 = 1024 #Configurable\n",
    "n_output = image_width * image_height * image_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"decoder\"):\n",
    "    hidden1 = tf.layers.dense(decoder_input, n_hidden1,\n",
    "                              activation=tf.nn.relu,\n",
    "                              name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2,\n",
    "                              activation=tf.nn.relu,\n",
    "                              name=\"hidden2\")\n",
    "    decoder_output = tf.layers.dense(hidden2, n_output,\n",
    "                                     activation=tf.nn.sigmoid,\n",
    "                                     name=\"decoder_output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the reconstruction loss. It is just the squared difference between the input image and the reconstructed image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_flat = tf.reshape(X, [-1, n_output], name=\"X_flat\")\n",
    "squared_difference = tf.square(X_flat - decoder_output,\n",
    "                               name=\"squared_difference\")\n",
    "reconstruction_loss = tf.reduce_sum(squared_difference,\n",
    "                                    name=\"reconstruction_loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final loss is the sum of the margin loss and the reconstruction loss (scaled down by a factor of 0.0005 to ensure the margin loss dominates training):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.0005\n",
    "\n",
    "loss = tf.add(margin_loss, alpha * reconstruction_loss, name=\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Touches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To measure our model's accuracy, we need to count the number of instances that are properly classified. For this, we can simply compare `y` and `y_pred`, convert the boolean value to a float32 (0.0 for False, 1.0 for True), and compute the mean over all the instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = tf.equal(y, y_pred, name=\"correct\")\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper mentions that the authors used the Adam optimizer with TensorFlow's default parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = tf.placeholder(tf.float32,name=\"learning_rate\")\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss, name=\"training_op\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init and Saver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's add the usual variable initializer, as well as a `Saver`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And... we're done with the construction phase! Please take a moment to celebrate. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainable Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_parameters = 0\n",
    "for variable in tf.trainable_variables():\n",
    "    # shape is an array of tf.Dimension\n",
    "    shape = variable.get_shape()\n",
    "    print(shape)\n",
    "    print(len(shape))\n",
    "    variable_parameters = 1\n",
    "    for dim in shape:\n",
    "        print(dim)\n",
    "        variable_parameters *= dim.value\n",
    "    print(variable_parameters)\n",
    "    total_parameters += variable_parameters\n",
    "print(total_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training our capsule network is pretty standard. For simplicity, we won't do any fancy hyperparameter tuning, dropout or anything, we will just run the training operation over and over again, displaying the loss, and at the end of each epoch, measure the accuracy on the validation set, display it, and save the model if the validation loss is the lowest seen found so far (this is a basic way to implement early stopping, without actually stopping). Hopefully the code should be self-explanatory, but here are a few details to note:\n",
    "* if a checkpoint file exists, it will be restored (this makes it possible to interrupt training, then restart it later from the last checkpoint),\n",
    "* we must not forget to feed `mask_with_labels=True` during training,\n",
    "* during testing, we let `mask_with_labels` default to `False` (but we still feed the labels since they are required to compute the accuracy),\n",
    "* the images loaded _via_ `mnist.train.next_batch()` are represented as `float32` arrays of shape \\[784\\], but the input placeholder `X` expects a `float32` array of shape \\[28, 28, 1\\], so we must reshape the images before we feed them to our model,\n",
    "* we evaluate the model's loss and accuracy on the full validation set (5,000 instances). To view progress and support systems that don't have a lot of RAM, the code evaluates the loss and accuracy on one batch at a time, and computes the mean loss and mean accuracy at the end.\n",
    "\n",
    "*Warning*: if you don't have a GPU, training will take a very long time (at least a few hours). With a GPU, it should take just a few minutes per epoch (e.g., 6 minutes on an NVidia GeForce GTX 1080Ti)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 3\n",
    "batch_size = 10\n",
    "keep_prob = 0.7\n",
    "learn_rate =0.0001\n",
    "restore_checkpoint = True\n",
    "num_train_examples = 10#len(X_train)\n",
    "num_valid_examples = 10#len(X_valid)\n",
    "\n",
    "n_iterations_per_epoch = num_train_examples // batch_size\n",
    "n_iterations_validation = num_valid_examples // batch_size\n",
    "\n",
    "best_loss_val = np.infty\n",
    "checkpoint_path = \"./Capsnet_Model_FC_Rev3_OrigionalRecon/my_capsule_network\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "with tf.Session() as sess:\n",
    "    if restore_checkpoint and tf.train.checkpoint_exists(checkpoint_path):\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        init.run()\n",
    "    \n",
    "    num_examples = len(X_train)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        for offset in range(0, num_train_examples, batch_size):\n",
    "            end = offset + batch_size\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            # Run the training operation and measure the loss:\n",
    "            _, loss_train = sess.run([training_op, loss],\n",
    "                feed_dict={learning_rate:learn_rate,\n",
    "                            X: batch_x.reshape([-1, image_width, image_height, image_channel]),\n",
    "                           y: batch_y,\n",
    "                           mask_with_labels: True,\n",
    "                           conv1_keep_prob: keep_prob})\n",
    "            iteration=offset//batch_size\n",
    "            print(\"\\rIteration: {}/{} ({:.1f}%)  Loss: {:.5f}\".format(\n",
    "                      iteration, n_iterations_per_epoch,\n",
    "                      iteration * 100 / n_iterations_per_epoch,\n",
    "                      loss_train), end=\"\")\n",
    "\n",
    "        # At the end of each epoch,\n",
    "        # measure the validation loss and accuracy:\n",
    "        \n",
    "        loss_vals = []\n",
    "        acc_vals = []\n",
    "        for offset in range(0, num_valid_examples, batch_size):\n",
    "            end = offset + batch_size\n",
    "            X_batch, y_batch = X_valid[offset:end], y_valid[offset:end]\n",
    "            \n",
    "            loss_val, acc_val = sess.run([loss, accuracy],\n",
    "                    feed_dict={X: X_batch.reshape([-1, image_width, image_height, image_channel]),\n",
    "                               y: y_batch,\n",
    "                               conv1_keep_prob: 1.0})\n",
    "            \n",
    "            loss_vals.append(loss_val)\n",
    "            acc_vals.append(acc_val)\n",
    "            iteration=offset//batch_size\n",
    "            print(\"\\rEvaluating the model: {}/{} ({:.1f}%)\".format(\n",
    "                      iteration, n_iterations_validation,\n",
    "                      iteration * 100 / n_iterations_validation),\n",
    "                  end=\" \" * 10)\n",
    "        loss_val = np.mean(loss_vals)\n",
    "        acc_val = np.mean(acc_vals)\n",
    "        print(\"\\rEpoch: {}  Val accuracy: {:.4f}%  Loss: {:.6f}{}\".format(\n",
    "            epoch + 1, acc_val * 100, loss_val,\n",
    "            \" (improved)\" if loss_val < best_loss_val else \"\"))\n",
    "\n",
    "        # And save the model if it improved:\n",
    "        \n",
    "        if loss_val < best_loss_val:\n",
    "            save_path = saver.save(sess, checkpoint_path)\n",
    "            best_loss_val = loss_val\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "num_test_examples = 10 #len(X_test)//400\n",
    "n_iterations_test = num_test_examples // batch_size\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, checkpoint_path)\n",
    "    loss_tests = []\n",
    "    acc_tests = []\n",
    "    \n",
    "    for offset in range(0, num_test_examples, batch_size):\n",
    "        end = offset + batch_size\n",
    "        X_batch, y_batch = X_test[offset:end], y_test[offset:end]\n",
    "        loss_test, acc_test = sess.run([loss, accuracy],\n",
    "                feed_dict={X: X_batch.reshape([-1, image_width, image_height, image_channel]),\n",
    "                           y: y_batch,\n",
    "                           conv1_keep_prob: 1.0})\n",
    "        loss_tests.append(loss_test)\n",
    "        acc_tests.append(acc_test)\n",
    "        #print(\"loss\",loss_test,\"accuracy\",acc_test)\n",
    "        iteration=offset//batch_size\n",
    "        print(\"\\rEvaluating the model: {}/{} ({:.1f}%)\".format(\n",
    "                  iteration, n_iterations_test,\n",
    "                  iteration * 100 / n_iterations_test),\n",
    "              end=\" \" * 10)\n",
    "    loss_test = np.mean(loss_tests)\n",
    "    acc_test = np.mean(acc_tests)\n",
    "    print(\"\\rFinal test accuracy: {:.4f}%  Loss: {:.6f}\".format(\n",
    "        acc_test * 100, loss_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make some predictions! We first fix a few images from the test set, then we start a session, restore the trained model, evaluate `caps2_output` to get the capsule network's output vectors, `decoder_output` to get the reconstructions, and `y_pred` to get the class predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 7\n",
    "\n",
    "data_set = X_test\n",
    "data_label = y_test\n",
    "maximum_index = len(data_set)\n",
    "indexes = np.random.randint(low=0,high=maximum_index,size = n_samples )\n",
    "\n",
    "sample_images = data_set[indexes].reshape([-1, image_width, image_width, image_channel])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, checkpoint_path)\n",
    "    caps2_output_value, decoder_output_value, y_pred_value = sess.run(\n",
    "            [caps2_output, decoder_output, y_pred],\n",
    "            feed_dict={X: sample_images,\n",
    "                       y: np.array([], dtype=np.int64),\n",
    "                       conv1_keep_prob: 1.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: we feed `y` with an empty array, but TensorFlow will not use it, as explained earlier.\n",
    "\n",
    "And now let's plot the images and their labels, followed by the corresponding reconstructions and predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_images = sample_images.reshape(-1, image_width, image_height, image_channel)\n",
    "reconstructions = decoder_output_value.reshape([-1, image_width, image_height, image_channel])\n",
    "\n",
    "plt.figure(figsize=(n_samples * 2, 3))\n",
    "for index, j in zip(range(n_samples),np.nditer(indexes)):\n",
    "    plt.subplot(1, n_samples, index + 1)\n",
    "    plt.imshow(sample_images[index], cmap=\"binary\")\n",
    "    plt.title(\"Label:\" + str(data_label[j]))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(n_samples * 2, 3))\n",
    "for index in range(n_samples):\n",
    "    plt.subplot(1, n_samples, index + 1)\n",
    "    plt.title(\"Predicted:\" + str(y_pred_value[index]))\n",
    "    plt.imshow(reconstructions[index], cmap=\"binary\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting the Output Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's tweak the output vectors to see what their pose parameters represent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's check the shape of the `cap2_output_value` NumPy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caps2_output_value.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function that will tweak each of the 16 pose parameters (dimensions) in all output vectors. Each tweaked output vector will be identical to the original output vector, except that one of its pose parameters will be incremented by a value varying from -0.5 to 0.5. By default there will be 11 steps (-0.5, -0.4, ..., +0.4, +0.5). This function will return an array of shape (_tweaked pose parameters_=16, _steps_=11, _batch size_=5, 1, 10, 16, 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweak_pose_parameters(output_vectors, min=-0.5, max=0.5, n_steps=11):\n",
    "    steps = np.linspace(min, max, n_steps) # -0.25, -0.15, ..., +0.25\n",
    "    pose_parameters = np.arange(caps2_n_dims) # 0, 1, ..., 15\n",
    "    tweaks = np.zeros([caps2_n_dims, n_steps, 1, 1, 1, caps2_n_dims, 1])\n",
    "    tweaks[pose_parameters, :, 0, 0, 0, pose_parameters, 0] = steps\n",
    "    output_vectors_expanded = output_vectors[np.newaxis, np.newaxis]\n",
    "    return tweaks + output_vectors_expanded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute all the tweaked output vectors and reshape the result to (_parameters_×_steps_×_instances_, 1, 10, 16, 1) so we can feed the array to the decoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 11\n",
    "\n",
    "tweaked_vectors = tweak_pose_parameters(caps2_output_value, n_steps=n_steps)\n",
    "tweaked_vectors_reshaped = tweaked_vectors.reshape(\n",
    "    [-1, 1, caps2_n_caps, caps2_n_dims, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_samples = 7\n",
    "\n",
    "#data_set = X_test\n",
    "#data_label = y_test\n",
    "#maximum_index = len(data_set)\n",
    "#indexes = np.random.randint(low=0,high=maximum_index,size = n_samples )\n",
    "#sample_images = data_set[indexes].reshape([-1, image_width, image_width, image_channel])\n",
    "\n",
    "tweak_labels = np.tile(data_label[indexes], caps2_n_dims * n_steps)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, checkpoint_path)\n",
    "    decoder_output_value = sess.run(\n",
    "            decoder_output,\n",
    "            feed_dict={caps2_output: tweaked_vectors_reshaped,\n",
    "                       mask_with_labels: True,\n",
    "                       y: tweak_labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reshape the decoder's output so we can easily iterate on the output dimension, the tweak steps, and the instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweak_reconstructions = decoder_output_value.reshape(\n",
    "        [caps2_n_dims, n_steps, n_samples, image_width, image_height,image_channel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for dim in range(caps2_n_dims):\n",
    "    print(\"Tweaking output dimension #{}\".format(dim))\n",
    "    plt.figure(figsize=(n_steps / 1.0, n_samples / 1.0))\n",
    "    \n",
    "    for row in range(n_samples):\n",
    "        for col in range(n_steps):        \n",
    "            plt.subplot(n_samples, n_steps, row * n_steps + col + 1)\n",
    "            plt.imshow(tweak_reconstructions[dim, col, row], cmap=\"binary\")\n",
    "            plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
